{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from PyPDF2 import PdfFileReader\n",
    "import pandas as pd\n",
    "import numby as np\n",
    "#loop in files in folder\n",
    "import glob\n",
    "#preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "#keyword libraries\n",
    "import gensim\n",
    "#!pip install rake-nltk\n",
    "from rake_nltk import Rake\n",
    "#!pip install git+https://github.com/LIAAD/yake\n",
    "import yake\n",
    "#!pip install git+https://github.com/boudinfl/pke.git\n",
    "import pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop in pdf fils in folder E:\\books\n",
    "BookTitles=[]\n",
    "contents=[]\n",
    "for filepath in glob.iglob(r'E:\\books\\*.pdf'):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        pdf = PdfFileReader(fp)\n",
    "        BookTitle=pdf.getDocumentInfo().title\n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser)\n",
    "        outlines = document.get_outlines()\n",
    "\n",
    "        content_title=\"\"\n",
    "        for(level,title,dest,a,se) in outlines:\n",
    "            content_title = content_title + title + \", \"\n",
    "\n",
    "        BookTitles.append(BookTitle)\n",
    "        contents.append(content_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(list(zip(BookTitles,contents)))\n",
    "df = pd.DataFrame(data, columns = ['BookTitle', 'TableContents']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BookTitle</th>\n",
       "      <th>TableContents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hands-On Machine Learning with Scikit-Learn an...</td>\n",
       "      <td>Cover, Copyright, Table of Contents, Preface, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Table of Contents, About the Author, About the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Table of Contents, About the Authors, About th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>List of Figures, List of Figures, List of Figu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Table of Contents, About the Author, About the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           BookTitle  \\\n",
       "0  Hands-On Machine Learning with Scikit-Learn an...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                       TableContents  \n",
       "0  Cover, Copyright, Table of Contents, Preface, ...  \n",
       "1  Table of Contents, About the Author, About the...  \n",
       "2  Table of Contents, About the Authors, About th...  \n",
       "3  List of Figures, List of Figures, List of Figu...  \n",
       "4  Table of Contents, About the Author, About the...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "##Creating a list of custom stopwords\n",
    "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
    "             \"show\", \"result\", \"large\", \n",
    "             \"first\",\"second\" ,\"third\",\"one\", \"two\", \"three\", \n",
    "             \"four\", \"five\", \"seven\",\"eight\",\"nine\",\"chapter\",\"cover\",\n",
    "             \"copyright\", \"table\", \"content\", \"preface\",\"author\",\"part\"\n",
    "             ,\"problem\", \"solution\", \"discussion\", \"see\", \"also\",\"section\",\"overview\",\n",
    "             \"introduction\",\"acknowledgment\",\"edition\",\"list\",\"summary\", \"exercise\"]\n",
    "stop_words = list(stop_words.union(new_words))\n",
    "\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    # lemmatize\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = [lmtzr.lemmatize(word) for word in text]\n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    # remove words less than three letters\n",
    "    text = [word for word in text if len(word) >= 3]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CleanTableContents'] = df['TableContents'].apply(lambda x:pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword Extraction by (gensim, rake_nltk, yake) \n",
    "\n",
    "And Keyphrases extraction using pke (TopicRank,SingleRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ann', 'file', 'installed', 'cnn', 'model', 'feature', 'application', 'recognition', 'layer', 'optimize', 'trained', 'weight', 'project', 'python implementation']\n"
     ]
    }
   ],
   "source": [
    "#gensim\n",
    "def gensim_model(text):\n",
    "    keywords=gensim.summarization.keywords(text, \n",
    "             ratio=0.5,               # use 50% of original text\n",
    "             words=15,              # Number of returned words\n",
    "             split=True,              # Whether split keywords\n",
    "             scores=False,            # Whether score of keyword\n",
    "             pos_filter=('NN', 'JJ'), # Part of speech (nouns, adjectives etc.) filters\n",
    "             lemmatize=True,         # If True - lemmatize words\n",
    "             deacc=True)              # If True - remove accentuation\n",
    "    return keywords\n",
    "print(gensim_model(df['CleanTableContents'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(35.5, 'deploying trained model using fruits 360 dataset'), (34.0, 'python locate libraries ?, manual installation'), (31.2, 'python installers locate libraries ?, preparing'), (31.166666666666664, 'backpropagation algorithm important ?, forward vs'), (28.5, 'deploying trained model using cifar10 dataset'), (24.166666666666668, 'fruits 360 dataset feature mining'), (19.666666666666664, 'image analysis using fc network'), (16.333333333333332, 'building android application using buildozer'), (16.0, 'projects using pip installer'), (15.666666666666666, 'basic application using boxlayout')]\n"
     ]
    }
   ],
   "source": [
    "#Automatic Keyword Extraction algorithm (RAKE) using NLTK\n",
    "#Doesn't work with preprocessed data\n",
    "def rake_model(text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    keywords= r.get_ranked_phrases_with_scores()[:10]\n",
    "    return keywords\n",
    "print(rake_model(df['TableContents'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Before cleaning Data-------------------------\n",
      "[('hidden layer', 0.0001876858385499582), ('trained model', 0.0003367550481360212), ('python implementation', 0.00043387377481509255), ('deploying trained', 0.000539293862029752), ('feature mining', 0.0005633338619663957), ('partial derivatives', 0.0005785845502473087), ('engineered features', 0.0005854036585875834), ('image recognition', 0.0006345839249222994), ('neural networks', 0.0006400209937720095), ('complete code', 0.0006400209937720095)]\n",
      "---------------------After cleaning Data-----------------------\n",
      "[('hidden layer', 0.0017542211782990385), ('partial derivative', 0.0021070977872515665), ('test pypi', 0.002407176680042636), ('engineered feature', 0.0024851508133507984), ('trained model', 0.0025873971442424057), ('python implementation', 0.0030798748576603024), ('pooling operation', 0.00316064668087735), ('importing module', 0.0032586473137344294), ('max pooling', 0.0033003471701552574), ('deploying trained', 0.0033946262875124682)]\n"
     ]
    }
   ],
   "source": [
    "#Keyword Extractor (Yake)\n",
    "def get_keywords_yake(text):\n",
    "    y = yake.KeywordExtractor(lan='en',          # language\n",
    "                             n = 2,              # n-gram size\n",
    "                             dedupLim = 0.9,     # deduplicationthresold\n",
    "                             dedupFunc = 'seqm', #  deduplication algorithm\n",
    "                             windowsSize = 1,\n",
    "                             top = 10,           # number of keys\n",
    "                             features=None)           \n",
    "    keywords = y.extract_keywords(text)\n",
    "    return keywords\n",
    "print(\"-------------------Before cleaning Data-------------------------\")\n",
    "keywords = get_keywords_yake(df['TableContents'][1])\n",
    "print(keywords)\n",
    "print(\"---------------------After cleaning Data-----------------------\")\n",
    "keywords2 = get_keywords_yake(df['CleanTableContents'][1])\n",
    "print(keywords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Before cleaning Data-------------------------\n",
      "[('anns', 0.043192441566586595), ('cnn', 0.032480254166656966), ('chapter', 0.029776977101706462), ('hidden layer', 0.02777480258548022), ('linear models', 0.02732021426843336), ('feature extraction', 0.02557925079801863), ('introduction', 0.02288111247268137), ('recognition', 0.02245413820988211), ('file upload', 0.0198146853837059), ('ann optimization', 0.019170460282572604), ('filter example', 0.018820747501588218), ('weight optimization', 0.01792119802480089), ('python implementation', 0.01656274541673227), ('backpropagation', 0.016244422789789853), ('tf core', 0.015713363777847038)]\n",
      "---------------------After cleaning Data-----------------------\n",
      "[('layer weight update', 0.08044962297618079), ('feature selection reduction filter wrapper', 0.0734962359728493), ('template dynamic template static file', 0.06457255648910591), ('chain partial derivative interpreting backpropagation', 0.05777641106636927), ('weight backpropagation', 0.057593032413143216), ('package', 0.04392085680742818), ('ann example ann', 0.04345761734380722), ('cnn model', 0.04129142856400024), ('flask application flask', 0.04072555481332895), ('equation backpropagation algorithm', 0.03784203362395949), ('understand', 0.037829824457243116), ('filter conv layer relu layer', 0.03772726827380543), ('regularization', 0.03488724175423734), ('max', 0.034005002398458946), ('important', 0.02849608708055694)]\n"
     ]
    }
   ],
   "source": [
    "#Keyphrases extraction using pke (TopicRank)\n",
    "def pke_TopicRank(text):\n",
    "    # 1. create a TopicRank extractor.\n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "    # 2. load the content of the document.\n",
    "    extractor.load_document(input=text)\n",
    "    # 3. select the longest sequences of nouns and adjectives, that do\n",
    "    #    not contain punctuation marks or stopwords as candidates.\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    stoplist += stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "    # 4. build topics by grouping candidates with HAC (average linkage,\n",
    "    #    threshold of 1/4 of shared stems). Weight the topics using random\n",
    "    #    walk, and select the first occuring candidate from each topic.\n",
    "    extractor.candidate_weighting(threshold=0.80, method='average')\n",
    "    # 5. get the 10-highest scored candidates as keyphrases\n",
    "    keyphrases = extractor.get_n_best(n=15)\n",
    "    return keyphrases\n",
    "print(\"-------------------Before cleaning Data-------------------------\")\n",
    "keywords = pke_TopicRank(df['TableContents'][1])\n",
    "print(keywords)\n",
    "print(\"---------------------After cleaning Data-----------------------\")\n",
    "keywords2 = pke_TopicRank(df['CleanTableContents'][1])\n",
    "print(keywords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Before cleaning Data-------------------------\n",
      "[('building android application using buildozer', 0.041164332459831976), ('image analysis using fc network', 0.03441072097251105), ('ann using ga', 0.03276208120598136), ('complete python implementation', 0.031025182217785476), ('ann implementation', 0.029133072372588255), ('tensorflow recognition application', 0.028216672032801718), ('image recognition pipeline', 0.028174204279277165), ('cnn model', 0.02623059307219868), ('kivy application life cycle', 0.02585740486818981), ('image recognition', 0.025472977686913964), ('ann optimization', 0.025467504307264466), ('training ann', 0.023708015230337613), ('graph visualization using tb', 0.023463259914996537), ('simple python project', 0.02345673550145131), ('python implementation', 0.02321515234085222)]\n",
      "---------------------After cleaning Data-----------------------\n",
      "[('rate training ann filter example ann architecture activation function python implementation learning rate testing network weight optimization backpropagation backpropagation', 0.1855983936797107), ('distance tournament selection crossover mutation optimizing ann complete python implementation convolutional neural network ann cnn intuition', 0.15659901936540607), ('buildozer spec file building android application buildozer recognition android cnn android appendix', 0.1265080687863979), ('best parent selection variation operator crossover mutation python implementation example complete implementation nsga nsga step dominance', 0.12156442200878695), ('hidden layer example ann single hidden layer recognition ann', 0.12080174339059462), ('directory doe python locate library manual installation copying project file site package python installers locate library', 0.11818687201730384), ('technical reviewer recognition computer vision recognition pipeline feature extraction color histogram histogram real world hsv', 0.10431665076879193), ('api locating parameter optimize building ffnn linear classification nonlinear classification cifar recognition cnn', 0.09821239987023266), ('layer complete code tensorflow recognition application tensor core dataflow graph tensor name', 0.09774200753761364), ('simple python project project structure project implementation running project', 0.08597466451692021), ('_ _ setup _ _ init _ _ setup', 0.0846500480153683), ('graph placeholder variable variable initialization graph visualization linear model optimizer train', 0.08028788773611784), ('training data building cnn training cnn', 0.07459003589347968), ('feature ann optimization optimization single multiobjective optimization', 0.0743595186440169), ('regularization artificial neural network anns linear model base anns', 0.06628765775708118)]\n"
     ]
    }
   ],
   "source": [
    "#Keyphrases extraction using pke (SingleRank)\n",
    "def pke_SingleRank(text):\n",
    "    # define the set of valid Part-of-Speeches\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "    # 1. create a SingleRank extractor.\n",
    "    extractor = pke.unsupervised.SingleRank()\n",
    "    # 2. load the content of the document.\n",
    "    extractor.load_document(input=text,language='en',normalization=None)\n",
    "    # 3. select the longest sequences of nouns and adjectives as candidates.\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "    # 4. weight the candidates using the sum of their word's scores that are computed using random walk. \n",
    "    #In the graph, nodes are words of certain part-of-speech (nouns and adjectives) that are connected if\n",
    "    # they occur in a window of 10 words.\n",
    "    extractor.candidate_weighting(window=10,pos=pos)\n",
    "    # 5. get the 10-highest scored candidates as keyphrases\n",
    "    keyphrases = extractor.get_n_best(n=15)\n",
    "    return keyphrases\n",
    "print(\"-------------------Before cleaning Data-------------------------\")\n",
    "keywords = pke_SingleRank(df['TableContents'][1])\n",
    "print(keywords)\n",
    "print(\"---------------------After cleaning Data-----------------------\")\n",
    "keywords2 = pke_SingleRank(df['CleanTableContents'][1])\n",
    "print(keywords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
